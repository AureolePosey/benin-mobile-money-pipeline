# ğŸ‡§ğŸ‡¯ Benin Mobile Money Data Pipeline (PySpark)

## ğŸ“Œ Project Overview

This project simulates and processes Mobile Money transaction data in Benin using PySpark.  
It demonstrates a complete end-to-end Data Engineering pipeline including:

- Data Generation
- Data Ingestion
- Data Validation (Data Quality checks)
- Data Transformation & Aggregation
- Writing optimized Parquet datasets

The goal is to showcase practical Data Engineering skills using Spark.

---

## ğŸ— Architecture

The pipeline is structured as follows:

1. **Data Generation**
   - Synthetic users dataset
   - Synthetic transactions dataset

2. **Ingestion**
   - Reading CSV files using Spark
   - Schema inference

3. **Validation**
   - Null checks
   - Business rule validation (negative amounts, future transactions)
   - Referential integrity (orphan transactions)

4. **Transformation**
   - Transaction volume per region
   - Top 10 users by transaction volume
   - Monthly & yearly aggregations

5. **Storage**
   - Results saved in Parquet format for optimized analytics

---

## ğŸ“‚ Project Structure
benin-mobile-money-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â””â”€â”€ processed/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ generate_data.py
â”‚ â”œâ”€â”€ ingest.py
â”‚ â”œâ”€â”€ validate.py
â”‚ â””â”€â”€ transform.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md



---

## âš™ï¸ How to Run

### 1ï¸âƒ£ Create virtual environment

```bash
python -m venv venv
source venv/bin/activate


2ï¸âƒ£ Install dependencies

pip install -r requirements.txt


3ï¸âƒ£ Run the pipeline


python src/generate_data.py
python src/ingest.py
python src/validate.py
python src/transform.py


ğŸ“Š Example Outputs

Transaction volume by region

Top 10 users by total transaction volume

Monthly transaction trends

All processed datasets are saved in:

data/processed/

ğŸ›  Technologies Used

Python

PySpark

Parquet

WSL (Windows Subsystem for Linux)



ğŸ¯ Key Learnings

Building a structured Data Engineering pipeline

Performing Data Quality validation with Spark

Business-driven aggregation logic

Writing optimized analytical datasets



ğŸ‘¤ Author

Rogelio Edjekpoto
Aspiring Data Engineer | Python | PySpark | SQL



## ğŸš€ Future Improvements

- Fraud detection logic
- Partitioned parquet optimization
- CI/CD integration
- Airflow orchestration